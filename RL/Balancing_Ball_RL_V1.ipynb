{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "gjobL-nozI81",
        "SJR1k4u6y9FH"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame-ce pymunk stable-baselines3 shimmy>=2.0"
      ],
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "712cf673-3313-4e24-e1f0-b9a0c096ee5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'=2.0'\t drive\t game_history   logs   models   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ],
      "metadata": {
        "id": "6bbqyvjZ1PZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313a2d38-7d13-42dd-8cfd-1a784d71908c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/capture': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recorder"
      ],
      "metadata": {
        "id": "hhEqO-xFu4AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "class Recorder:\n",
        "\n",
        "    def __init__(self, task: str = \"game_history_record\"):\n",
        "        \"\"\"\n",
        "        tasks:\n",
        "        1. game_history_record\n",
        "        2. temp_memory\n",
        "        \"\"\"\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \"\"\n",
        "        if task == \"game_history_record\":\n",
        "            collection_name = self.get_newest_record_name()\n",
        "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.json_file_path):\n",
        "            print(\"Loading the json memory file\")\n",
        "            self.memory = self.load(self.json_file_path)\n",
        "        else:\n",
        "            print(\"The json memory file does not exist. Creating new file.\")\n",
        "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
        "            with open(self.json_file_path, \"w\") as f:\n",
        "                json.dump(self.memory, f)\n",
        "\n",
        "    def get(self):\n",
        "        print(\"Getting the json memory\")\n",
        "        return self.memory\n",
        "\n",
        "    def add_no_limit(self, data: float, ):\n",
        "        \"\"\"\n",
        "        Add a records.\n",
        "\n",
        "        Args:\n",
        "            role: The role of the sender (e.g., 'user', 'assistant')\n",
        "            message: The message content\n",
        "        \"\"\"\n",
        "        self.memory[\"game_records\"].append({\n",
        "            \"game_total_duration\": data,\n",
        "            \"timestamp\": str(datetime.datetime.now())\n",
        "        })\n",
        "\n",
        "        self.save(self.json_file_path)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(self.memory, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving memory to {file_path}: {e}\")\n",
        "\n",
        "    def load(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading memory from {file_path}: {e}\")\n",
        "            return {\"game_records\": []}\n",
        "\n",
        "    def get_newest_record_name(self) -> str:\n",
        "        \"\"\"\n",
        "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
        "            - 例如: \"game_2022-01\"\n",
        "        \"\"\"\n",
        "\n",
        "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
        "        return \"record_\" + this_month"
      ],
      "metadata": {
        "id": "gJwfQb_Yuz1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shapes & Objects"
      ],
      "metadata": {
        "id": "cnA8wZtosmeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Shape:\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape: Optional[pymunk.Shape] = None,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a physical shape with associated body.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the body\n",
        "            velocity: Initial velocity (vx, vy) of the body\n",
        "            body: The pymunk Body to attach to this shape\n",
        "            shape: The pymunk Shape for collision detection\n",
        "        \"\"\"\n",
        "\n",
        "        self.body = body\n",
        "        self.default_position = position\n",
        "        self.default_velocity = velocity\n",
        "        self.body.position = position\n",
        "        self.body.velocity = velocity\n",
        "        self.default_angular_velocity = 0\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
        "        self.body.position = self.default_position\n",
        "        self.body.velocity = self.default_velocity\n",
        "        self.body.angular_velocity = self.default_angular_velocity\n"
      ],
      "metadata": {
        "id": "5wMhHMWCsmVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "\n",
        "# from shapes.shape import Shape\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Circle(Shape):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape_radio: float = 20,\n",
        "                shape_mass: float = 1,\n",
        "                shape_friction: float = 0.1,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a circular physics object.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the circle\n",
        "            velocity: Initial velocity (vx, vy) of the circle\n",
        "            body: The pymunk Body to attach this circle to\n",
        "            shape_radio: Radius of the circle in pixels\n",
        "            shape_mass: Mass of the circle\n",
        "            shape_friction: Friction coefficient for the circle\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(position, velocity, body)\n",
        "        self.shape_radio = shape_radio\n",
        "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
        "        self.shape.mass = shape_mass\n",
        "        self.shape.friction = shape_friction\n",
        "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
      ],
      "metadata": {
        "id": "mfw5tBxBswyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game class"
      ],
      "metadata": {
        "id": "v-8d5fKltI62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "import pygame\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "# from shapes.circle import Circle\n",
        "# from record import Recorder\n",
        "\n",
        "class BalancingBallGame:\n",
        "    \"\"\"\n",
        "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Game constants\n",
        "\n",
        "\n",
        "    # Visual settings for indie style\n",
        "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
        "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
        "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
        "    PARTICLE_COLORS = [(252, 186, 3), (252, 127, 3), (252, 3, 3)]  # Fire-like particles\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode: str = \"human\",\n",
        "                 sound_enabled: bool = True,\n",
        "                 difficulty: str = \"medium\",\n",
        "                 window_x: int = 1000,\n",
        "                 window_y: int = 600,\n",
        "                 max_step: int = 30000,\n",
        "                 reward_staying_alive: float = 0.1,\n",
        "                 reward_ball_centered: float = 0.2,\n",
        "                 penalty_falling: float = -10.0,\n",
        "                 fps: int = 120,\n",
        "                 platform_shape: str = \"circle\",\n",
        "                 platform_length: int = 200,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialize the balancing ball game.\n",
        "\n",
        "        Args:\n",
        "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
        "            sound_enabled: Whether to enable sound effects\n",
        "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
        "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
        "            reward_staying_alive: float = 0.1,\n",
        "            reward_ball_centered: float = 0.2,\n",
        "            penalty_falling: float = -10.0,\n",
        "            fps: frame per second\n",
        "        \"\"\"\n",
        "        # Game parameters\n",
        "        self.max_step = max_step\n",
        "        self.reward_staying_alive = reward_staying_alive\n",
        "        self.reward_ball_centered = reward_ball_centered\n",
        "        self.penalty_falling = penalty_falling\n",
        "        self.fps = fps\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "        self.recorder = Recorder(\"game_history_record\")\n",
        "        self.render_mode = render_mode\n",
        "        self.sound_enabled = sound_enabled\n",
        "        self.difficulty = difficulty\n",
        "\n",
        "        self._get_x_axis_max_reward_rate(platform_length)\n",
        "\n",
        "        # Initialize physics space\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = (0, 1000)\n",
        "        self.space.damping = 0.9\n",
        "\n",
        "        # Create game bodies\n",
        "        self.dynamic_body = pymunk.Body()  # Ball body\n",
        "        self.kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        self.kinematic_body.position = (self.window_x / 2, 400)\n",
        "        self.default_kinematic_position = self.kinematic_body.position\n",
        "\n",
        "        # Create game objects\n",
        "        self._create_ball()\n",
        "        self._create_platform(platform_shape=platform_shape, platform_length=platform_length)\n",
        "        # self._create_platform(\"rectangle\")\n",
        "\n",
        "        # Add all objects to space\n",
        "        self.space.add(self.dynamic_body, self.kinematic_body,\n",
        "                       self.circle.shape, self.platform)\n",
        "\n",
        "        # Game state tracking\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Initialize Pygame if needed\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            self._setup_pygame()\n",
        "\n",
        "        # Set difficulty parameters\n",
        "        self._apply_difficulty()\n",
        "\n",
        "        # Create folders for captures if needed\n",
        "        if render_mode == \"rgb_array\":\n",
        "            # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "            CURRENT_DIR = \"\"\n",
        "            os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
        "\n",
        "    def _setup_pygame(self):\n",
        "        \"\"\"Set up PyGame for rendering\"\"\"\n",
        "        pygame.init()\n",
        "        if self.sound_enabled:\n",
        "            self._load_sounds()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
        "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
        "        else:\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
        "\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.font = pygame.font.Font(None, 30)\n",
        "\n",
        "        # Create custom draw options for indie style\n",
        "\n",
        "    def _load_sounds(self):\n",
        "        \"\"\"Load game sound effects\"\"\"\n",
        "        try:\n",
        "            pygame.mixer.init()\n",
        "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
        "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
        "        except Exception:\n",
        "            print(\"Sound loading error\")\n",
        "            self.sound_enabled = False\n",
        "            pass\n",
        "    def _create_ball(self):\n",
        "        \"\"\"Create the ball with physics properties\"\"\"\n",
        "        self.ball_radius = 15\n",
        "        self.circle = Circle(\n",
        "            position=(self.window_x / 2, 200),\n",
        "            velocity=(0, 0),\n",
        "            body=self.dynamic_body,\n",
        "            shape_radio=self.ball_radius,\n",
        "            shape_friction=100,\n",
        "        )\n",
        "        # Store initial values for reset\n",
        "        self.default_ball_position = self.dynamic_body.position\n",
        "\n",
        "    def _create_platform(self,\n",
        "                         platform_shape: str = \"circle\",\n",
        "                         platform_length: int = 200\n",
        "                        ):\n",
        "        \"\"\"\n",
        "        Create the platform with physics properties\n",
        "        platform_shape: circle, rectangle\n",
        "        platform_length: Length of a rectangle or Diameter of a circle\n",
        "        \"\"\"\n",
        "        if platform_shape == \"circle\":\n",
        "            self.platform_length = platform_length / 2 # radius\n",
        "            self.platform = pymunk.Circle(self.kinematic_body, self.platform_length)\n",
        "            self.platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
        "            self.platform.friction = 0.7\n",
        "        elif platform_shape == \"rectangle\":\n",
        "            self.platform_length = 200\n",
        "            vs = [(-self.platform_length/2, -10),\n",
        "                (self.platform_length/2, -10),\n",
        "                (self.platform_length/2, 10),\n",
        "                (-self.platform_length/2, 10)]\n",
        "\n",
        "            self.platform = pymunk.Poly(self.kinematic_body, vs)\n",
        "        self.platform.friction = 0.7\n",
        "        self.platform_rotation = 0\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "    def _apply_difficulty(self):\n",
        "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
        "        if self.difficulty == \"easy\":\n",
        "            self.max_platform_speed = 1.5\n",
        "            self.ball_elasticity = 0.5\n",
        "        elif self.difficulty == \"medium\":\n",
        "            self.max_platform_speed = 2.5\n",
        "            self.ball_elasticity = 0.7\n",
        "        else:  # hard\n",
        "            self.max_platform_speed = 3.5\n",
        "            self.ball_elasticity = 0.9\n",
        "\n",
        "        self.circle.shape.elasticity = self.ball_elasticity\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
        "        # Reset physics objects\n",
        "        self.dynamic_body.position = self.default_ball_position\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "\n",
        "        self.kinematic_body.position = self.default_kinematic_position\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        # Reset game state\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Return initial observation\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Take a step in the game using the given action.\n",
        "\n",
        "        Args:\n",
        "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
        "\n",
        "        Returns:\n",
        "            observation: Game state observation\n",
        "            reward: Reward for this step\n",
        "            terminated: Whether episode is done\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # Apply action to platform rotation\n",
        "        self.dynamic_body.angular_velocity += action\n",
        "\n",
        "        # Step the physics simulation\n",
        "        self.space.step(1/self.fps)\n",
        "\n",
        "        # Update particle effects\n",
        "        self._update_particles()\n",
        "\n",
        "        # Check game state\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "        reward = self.reward_staying_alive\n",
        "\n",
        "        # Calculate reward for keeping ball centered on platform\n",
        "        ball_x = self.dynamic_body.position[0]\n",
        "\n",
        "        # Check if ball falls off screen\n",
        "        if (self.dynamic_body.position[1] > self.kinematic_body.position[1] or\n",
        "            self.dynamic_body.position[0] < 0 or\n",
        "            self.dynamic_body.position[0] > self.window_x or\n",
        "            self.steps >= self.max_step\n",
        "            ):\n",
        "\n",
        "            print(\"Score: \", self.score)\n",
        "            terminated = True\n",
        "            reward = self.penalty_falling if self.steps < self.max_step else 0\n",
        "            self.game_over = True\n",
        "\n",
        "            result = {\n",
        "                \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
        "                \"score\": self.score,\n",
        "            }\n",
        "            self.recorder.add_no_limit(result)\n",
        "\n",
        "            if self.sound_enabled and self.sound_fall:\n",
        "                self.sound_fall.play()\n",
        "\n",
        "        step_reward = self._reward_calculator(ball_x)\n",
        "        self.score += step_reward\n",
        "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
        "        return self._get_observation(), step_reward, terminated\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
        "        # # Normalize values to suitable ranges\n",
        "        # ball_x = self.dynamic_body.position[0] / self.window_x\n",
        "        # ball_y = self.dynamic_body.position[1] / self.window_y\n",
        "        # ball_vx = self.dynamic_body.velocity[0] / 1000\n",
        "        # ball_vy = self.dynamic_body.velocity[1] / 1000\n",
        "\n",
        "        # platform_angle = (self.kinematic_body.angle % (2*np.pi)) / (2*np.pi)\n",
        "        # platform_angular_velocity = self.kinematic_body.angular_velocity / self.max_platform_speed\n",
        "\n",
        "        # return np.array([\n",
        "        #     ball_x, ball_y, ball_vx, ball_vy,\n",
        "        #     platform_angle, platform_angular_velocity\n",
        "        # ], dtype=np.float32)\n",
        "\n",
        "\n",
        "        screen_data = pygame.surfarray.array3d(self.screen)  # 获取数据\n",
        "        screen_data = np.transpose(screen_data, (1, 0, 2))  # 转置以符合 (height, width, channels)\n",
        "\n",
        "        return screen_data\n",
        "\n",
        "    def _update_particles(self):\n",
        "        \"\"\"Update particle effects for indie visual style\"\"\"\n",
        "        # Create new particles when ball hits platform\n",
        "        if abs(self.dynamic_body.position[1] - (self.kinematic_body.position[1] - 20)) < 5 and abs(self.dynamic_body.velocity[1]) > 100:\n",
        "            for _ in range(5):\n",
        "                self.particles.append({\n",
        "                    'x': self.dynamic_body.position[0],\n",
        "                    'y': self.dynamic_body.position[1] + self.ball_radius,\n",
        "                    'vx': random.uniform(-2, 2),\n",
        "                    'vy': random.uniform(1, 3),\n",
        "                    'life': 30,\n",
        "                    'size': random.uniform(2, 5),\n",
        "                    'color': random.choice(self.PARTICLE_COLORS)\n",
        "                })\n",
        "\n",
        "            if self.sound_enabled and self.sound_bounce:\n",
        "                self.sound_bounce.play()\n",
        "\n",
        "        # Update existing particles\n",
        "        for particle in self.particles[:]:\n",
        "            particle['x'] += particle['vx']\n",
        "            particle['y'] += particle['vy']\n",
        "            particle['life'] -= 1\n",
        "            if particle['life'] <= 0:\n",
        "                self.particles.remove(particle)\n",
        "\n",
        "    def render(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Render the current game state\"\"\"\n",
        "        if self.render_mode == \"headless\":\n",
        "            return None\n",
        "\n",
        "        # Clear screen with background color\n",
        "        self.screen.fill(self.BACKGROUND_COLOR)\n",
        "\n",
        "        # Custom drawing (for indie style)\n",
        "        self._draw_indie_style()\n",
        "\n",
        "        # Draw game information\n",
        "        self._draw_game_info()\n",
        "\n",
        "        # Update display if in human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.fps)\n",
        "            return None\n",
        "        else:\n",
        "            # Return RGB array for gym environment\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        # platform_points = []\n",
        "        # for v in self.platform.get_vertices():\n",
        "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
        "        #     platform_points.append((int(x), int(y)))\n",
        "\n",
        "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "\n",
        "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "        # Draw rotation direction indicator\n",
        "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
        "\n",
        "        # Draw ball with gradient and glow\n",
        "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "        # Draw particles\n",
        "        for particle in self.particles:\n",
        "            alpha = min(255, int(255 * (particle['life'] / 30)))\n",
        "            pygame.draw.circle(\n",
        "                self.screen,\n",
        "                particle['color'],\n",
        "                (int(particle['x']), int(particle['y'])),\n",
        "                int(particle['size'])\n",
        "            )\n",
        "\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = self.kinematic_body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
        "\n",
        "    def _draw_game_info(self):\n",
        "        \"\"\"Draw game information on screen\"\"\"\n",
        "        # Create texts\n",
        "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
        "        score_text = f\"Score: {self.score}\"\n",
        "\n",
        "        # Render texts\n",
        "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
        "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
        "\n",
        "        # Draw text backgrounds\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
        "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
        "\n",
        "        # Draw texts\n",
        "        self.screen.blit(time_surface, (10, 10))\n",
        "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
        "\n",
        "        # Draw game over screen\n",
        "        if self.game_over:\n",
        "            game_over_text = \"GAME OVER - Press R to restart\"\n",
        "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw semi-transparent background\n",
        "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
        "            overlay.fill((0, 0, 0, 128))\n",
        "            self.screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Draw text\n",
        "            self.screen.blit(game_over_surface,\n",
        "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
        "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
        "\n",
        "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
        "        \"\"\"\n",
        "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
        "        center of game window coordinates. The closer you are, the higher the reward.\n",
        "\n",
        "        When the ball is to be 10 points away from the center coordinates,\n",
        "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        \"\"\"\n",
        "        self.reward_width = (platform_length / 2) - 5\n",
        "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
        "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
        "\n",
        "    def _reward_calculator(self, ball_x):\n",
        "        # score & reward\n",
        "        if self.steps < 2000:\n",
        "            step_reward = self.steps * 0.01\n",
        "        elif self.steps < 5000:\n",
        "            step_reward = self.steps * 0.03\n",
        "        else:\n",
        "            step_reward = self.steps * 0.05\n",
        "\n",
        "        rw = abs(ball_x - self.window_x/2)\n",
        "        if rw < self.reward_width:\n",
        "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "            return step_reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the game and clean up resources\"\"\"\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            pygame.quit()\n",
        "\n",
        "    def run_standalone(self):\n",
        "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
        "        # if self.render_mode not in [\"human\"]:\n",
        "        #     raise ValueError(\"Standalone mode requires render_mode='human'\")\n",
        "\n",
        "        running = True\n",
        "        while running:\n",
        "            # Handle events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_r and self.game_over:\n",
        "                        self.reset()\n",
        "\n",
        "            # Process keyboard controls\n",
        "            keys = pygame.key.get_pressed()\n",
        "            action = 0\n",
        "            if keys[pygame.K_LEFT]:\n",
        "                action = -1.0\n",
        "            if keys[pygame.K_RIGHT]:\n",
        "                action = 1.0\n",
        "\n",
        "            # Take game step\n",
        "            if not self.game_over:\n",
        "                self.step(action)\n",
        "\n",
        "            # Render\n",
        "            self.render()\n",
        "\n",
        "        self.close()"
      ],
      "metadata": {
        "id": "-wiw5Rjks-xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a7f927-ce48-46f6-aaa8-32134c464f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame-ce 2.5.3 (SDL 2.30.12, Python 3.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GYM env"
      ],
      "metadata": {
        "id": "gjobL-nozI81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "class BalancingBallEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    OpenAI Gym environment for the Balancing Ball game\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "\n",
        "    def __init__(self, render_mode=\"rgb_array\", difficulty=\"medium\", fps=30):\n",
        "        super(BalancingBallEnv, self).__init__()\n",
        "\n",
        "        # Action space: discrete - 0: left, 1: right\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Initialize game\n",
        "        self.window_x = 1000\n",
        "        self.window_y = 600\n",
        "        self.platform_shape = \"circle\"\n",
        "        self.platform_length = 200\n",
        "\n",
        "        self.game = BalancingBallGame(\n",
        "            render_mode=render_mode,\n",
        "            sound_enabled=(render_mode == \"human\"),\n",
        "            difficulty=difficulty,\n",
        "            window_x = self.window_x,\n",
        "            window_y = self.window_y,\n",
        "            platform_shape = self.platform_shape,\n",
        "            platform_length = self.platform_length,\n",
        "            fps = fps,\n",
        "        )\n",
        "\n",
        "        # Image observation space (RGB)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255,\n",
        "            shape=(self.window_y, self.window_x, 3),\n",
        "            dtype=np.uint8\n",
        "        )\n",
        "\n",
        "        # Platform_length /= 2 when for calculate the distance to the\n",
        "        # center of game window coordinates. The closer you are, the higher the reward.\n",
        "        self.platform_length = (self.platform_length / 2) - 5\n",
        "\n",
        "        # When the ball is to be 10 points away from the center coordinates,\n",
        "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        self.x_axis_max_reward_rate = 0.5 / self.platform_length\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Convert from discrete action to the game's expected format\n",
        "        action_value = -1.0 if action == 0 else 1.0\n",
        "\n",
        "        # Take step in the game\n",
        "        obs, step_reward, terminated = self.game.step(action_value)\n",
        "\n",
        "        # OpenAI Gym expects (observation, reward, terminated, truncated, info)\n",
        "        return obs, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        observation = self.game.reset()\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        return self.game.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.game.close()"
      ],
      "metadata": {
        "id": "MBzvHTN1zJu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "SJR1k4u6y9FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(difficulty=\"medium\"):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "    window_x = 1000\n",
        "    window_y = 600\n",
        "    platform_shape = \"circle\"\n",
        "    platform_length = 200\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        difficulty=difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        platform_shape = platform_shape,\n",
        "        platform_length = platform_length,\n",
        "        fps = 30,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "    \"\"\"Test the OpenAI Gym environment\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"human\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample a random action (for testing only)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # parser = argparse.ArgumentParser(description='Run Balancing Ball game')\n",
        "    # parser.add_argument('--mode', type=str, default='gym',\n",
        "    #                     choices=['game', 'gym'],\n",
        "    #                     help='Mode to run: standalone game or gym environment test')\n",
        "    # parser.add_argument('--difficulty', type=str, default='medium',\n",
        "    #                     choices=['easy', 'medium', 'hard'],\n",
        "    #                     help='Game difficulty level')\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # if args.mode == 'game':\n",
        "    #     run_standalone_game(difficulty=args.difficulty)\n",
        "    # else:\n",
        "    #     test_gym_env(difficulty=args.difficulty)\n",
        "    # run_standalone_game(difficulty=\"medium\")\n",
        "    test_gym_env(difficulty=\"medium\")"
      ],
      "metadata": {
        "id": "Uea1POIGtIff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1521bd78-a6d0-4798-bba5-f91420e97c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The json memory file does not exist. Creating new file.\n",
            "self.x_axis_max_reward_rate:  0.021052631578947368\n",
            "Sound loading error\n",
            "Score:  22.48110223647175\n",
            "Episode 1: Steps: 52, Total Reward: 22.48\n",
            "Score:  19.279474977499955\n",
            "Episode 2: Steps: 50, Total Reward: 19.28\n",
            "Score:  55.02027501148567\n",
            "Episode 3: Steps: 75, Total Reward: 55.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "90pbC8lLWB04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticCnnPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Add the game directory to the system path\n",
        "# sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"game_base_files_test\"))\n",
        "\n",
        "# from gym_env import BalancingBallEnv\n",
        "\n",
        "def make_env(render_mode=\"rgb_array\", difficulty=\"medium\"):\n",
        "    \"\"\"\n",
        "    Create and return an environment function to be used with VecEnv\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def train_ppo(\n",
        "    total_timesteps=1000000,\n",
        "    learning_rate=0.003,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    ent_coef=0.01,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    policy_kwargs=None,\n",
        "    n_envs=4,\n",
        "    save_freq=10000,\n",
        "    log_dir=\"./logs/\",\n",
        "    model_dir=\"./models/\",\n",
        "    eval_freq=10000,\n",
        "    eval_episodes=5,\n",
        "    difficulty=\"medium\",\n",
        "    load_model=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "    Args:\n",
        "        total_timesteps: Total number of steps to train for\n",
        "        n_envs: Number of parallel environments\n",
        "        save_freq: How often to save checkpoints (in timesteps)\n",
        "        log_dir: Directory for tensorboard logs\n",
        "        model_dir: Directory to save models\n",
        "        eval_freq: How often to evaluate the model (in timesteps)\n",
        "        eval_episodes: Number of episodes to evaluate on\n",
        "        difficulty: Game difficulty level\n",
        "        load_model: Path to model to load for continued training\n",
        "    \"\"\"\n",
        "    # Create directories\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Setup environments\n",
        "    env = make_vec_env(\n",
        "        make_env(difficulty=difficulty),\n",
        "        n_envs=n_envs\n",
        "    )\n",
        "\n",
        "    # Apply VecTransposeImage to correctly handle image observations\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    # Setup evaluation environment\n",
        "    eval_env = make_vec_env(\n",
        "        make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
        "        n_envs=1\n",
        "    )\n",
        "    eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "    # Define policy kwargs if not provided\n",
        "    if policy_kwargs is None:\n",
        "        policy_kwargs = {\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "        }\n",
        "\n",
        "    # Create the PPO model\n",
        "    if load_model:\n",
        "        print(f\"Loading model from {load_model}\")\n",
        "        model = PPO.load(\n",
        "            load_model,\n",
        "            env=env,\n",
        "            tensorboard_log=log_dir,\n",
        "        )\n",
        "    else:\n",
        "        model = PPO(\n",
        "            policy=ActorCriticCnnPolicy,\n",
        "            env=env,\n",
        "            learning_rate=learning_rate,\n",
        "            n_steps=n_steps,\n",
        "            batch_size=batch_size,\n",
        "            n_epochs=n_epochs,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            ent_coef=ent_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            tensorboard_log=log_dir,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "    # Setup callbacks\n",
        "    checkpoint_callback = CheckpointCallback(\n",
        "        save_freq=save_freq // n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "        save_path=model_dir,\n",
        "        name_prefix=\"ppo_balancing_ball\"\n",
        "    )\n",
        "\n",
        "    eval_callback = EvalCallback(\n",
        "        eval_env,\n",
        "        best_model_save_path=model_dir,\n",
        "        log_path=log_dir,\n",
        "        eval_freq=eval_freq // n_envs,\n",
        "        n_eval_episodes=eval_episodes,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[checkpoint_callback, eval_callback],\n",
        "    )\n",
        "\n",
        "    # Save the final model\n",
        "    model.save(f\"{model_dir}/ppo_balancing_ball_final\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n",
        "\n",
        "def evaluate(model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model\n",
        "        n_episodes: Number of episodes to evaluate on\n",
        "        difficulty: Game difficulty level\n",
        "    \"\"\"\n",
        "    # Create environment for evaluation\n",
        "    env = make_vec_env(\n",
        "        make_env(render_mode=\"human\", difficulty=difficulty),\n",
        "        n_envs=1\n",
        "    )\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    # Load the model\n",
        "    model = PPO.load(model_path)\n",
        "\n",
        "    # Evaluate\n",
        "    mean_reward, std_reward = evaluate_policy(\n",
        "        model,\n",
        "        env,\n",
        "        n_eval_episodes=n_episodes,\n",
        "        deterministic=True,\n",
        "        render=True\n",
        "    )\n",
        "\n",
        "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "  # import argparse\n",
        "\n",
        "  # parser = argparse.ArgumentParser(description=\"Train or evaluate PPO agent for Balancing Ball\")\n",
        "  # parser.add_argument(\"--mode\", type=str, default=\"train\", choices=[\"train\", \"eval\"],\n",
        "  #                     help=\"Mode: 'train' to train model, 'eval' to evaluate\")\n",
        "  # parser.add_argument(\"--timesteps\", type=int, default=1000000,\n",
        "  #                     help=\"Total timesteps for training\")\n",
        "  # parser.add_argument(\"--difficulty\", type=str, default=\"medium\",\n",
        "  #                     choices=[\"easy\", \"medium\", \"hard\"],\n",
        "  #                     help=\"Game difficulty\")\n",
        "  # parser.add_argument(\"--load_model\", type=str, default=None,\n",
        "  #                     help=\"Path to model to load for continued training or evaluation\")\n",
        "  # parser.add_argument(\"--n_envs\", type=int, default=4,\n",
        "  #                     help=\"Number of parallel environments for training\")\n",
        "  # parser.add_argument(\"--eval_episodes\", type=int, default=5,\n",
        "  #                     help=\"Number of episodes for evaluation\")\n",
        "\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "  # if args.mode == \"train\":\n",
        "  #     train_ppo(\n",
        "  #         total_timesteps=args.timesteps,\n",
        "  #         difficulty=args.difficulty,\n",
        "  #         n_envs=args.n_envs,\n",
        "  #         load_model=args.load_model,\n",
        "  #         eval_episodes=args.eval_episodes,\n",
        "  #     )\n",
        "  # else:\n",
        "  #     if args.load_model is None:\n",
        "  #         print(\"Error: Must provide --load_model for evaluation\")\n",
        "  #     else:\n",
        "  #         evaluate(\n",
        "  #             model_path=args.load_model,\n",
        "  #             n_episodes=args.eval_episodes,\n",
        "  #             difficulty=args.difficulty\n",
        "  #         )\n",
        "# train_ppo(\n",
        "#   total_timesteps=15000,\n",
        "#   n_steps=1024,\n",
        "#   batch_size=32,\n",
        "#   difficulty=\"medium\",\n",
        "#   n_envs=1,\n",
        "#   load_model=None,\n",
        "#   eval_episodes=2,\n",
        "# )\n",
        "\n",
        "train_ppo(\n",
        "  total_timesteps=10000,\n",
        "  n_steps=1024,\n",
        "  batch_size=32,\n",
        "  difficulty=\"medium\",\n",
        "  n_envs=1,\n",
        "  load_model=\"/content/models/ppo_balancing_ball_final.zip\",\n",
        "  eval_episodes=2,\n",
        ")\n",
        "train_ppo(\n",
        "  total_timesteps=10000,\n",
        "  n_steps=1024,\n",
        "  batch_size=32,\n",
        "  difficulty=\"medium\",\n",
        "  n_envs=1,\n",
        "  load_model=\"/content/models/ppo_balancing_ball_final.zip\",\n",
        "  eval_episodes=2,\n",
        ")\n",
        "train_ppo(\n",
        "  total_timesteps=10000,\n",
        "  n_steps=1024,\n",
        "  batch_size=32,\n",
        "  difficulty=\"medium\",\n",
        "  n_envs=1,\n",
        "  load_model=\"/content/models/ppo_balancing_ball_final.zip\",\n",
        "  eval_episodes=2,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTdFslcyWAT8",
        "outputId": "8d6c1aa8-a13a-4361-aa61-b025bcc31316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.021052631578947368\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.021052631578947368\n",
            "Loading model from /content/models/ppo_balancing_ball_final.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Logging to ./logs/PPO_1\n",
            "Score:  44.095710543701394\n",
            "Score:  17.78806357774433\n",
            "Score:  138.25766574542445\n",
            "Score:  20.48282559743561\n",
            "Score:  16.324828040199684\n",
            "Score:  30.960453373559186\n",
            "Score:  13.671523809391614\n",
            "Score:  14.91007149010525\n",
            "Score:  15.799400380353342\n",
            "Score:  66.46783971652083\n",
            "Score:  68.52188947328504\n",
            "Score:  18.985908564993355\n",
            "Score:  14.67935187376332\n",
            "Score:  14.510293559502534\n",
            "Score:  34.623157717942476\n",
            "Score:  19.345949816687487\n",
            "Score:  33.57295468054595\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 57.9     |\n",
            "|    ep_rew_mean     | 34.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 32       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 1024     |\n",
            "---------------------------------\n",
            "Score:  15.171266613513449\n",
            "Score:  30.84304590375227\n",
            "Score:  86.61393728648406\n",
            "Score:  19.330463743427515\n",
            "Score:  18.36019698496053\n",
            "Score:  19.747349757573392\n",
            "Score:  26.545048582716916\n",
            "Score:  19.295552848384858\n",
            "Score:  19.44106607120959\n",
            "Score:  67.39552771939712\n",
            "Score:  105.35824090373795\n",
            "Score:  31.110030725538174\n",
            "Score:  34.89652314949412\n",
            "Score:  14.19894792694352\n",
            "Score:  19.200171497158383\n",
            "Score:  18.280855009857742\n",
            "Score:  20.335976004478592\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 58.1        |\n",
            "|    ep_rew_mean          | 33.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 14          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 141         |\n",
            "|    total_timesteps      | 2048        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010531401 |\n",
            "|    clip_fraction        | 0.0329      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.678      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.003       |\n",
            "|    loss                 | 56.7        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00358    |\n",
            "|    value_loss           | 99.8        |\n",
            "-----------------------------------------\n",
            "Score:  53.68254002213261\n",
            "Score:  15.51904360618952\n",
            "Score:  15.377799557391628\n",
            "Score:  25.042047259393712\n",
            "Score:  20.749325080132998\n",
            "Score:  30.502964982148562\n",
            "Score:  30.525900497559896\n",
            "Score:  19.29454621562955\n",
            "Score:  38.816270864657156\n",
            "Score:  34.493309327588435\n",
            "Score:  19.504536758018407\n",
            "Score:  65.31155648084925\n",
            "Score:  21.86076373280909\n",
            "Score:  49.380120647812596\n",
            "Score:  49.16525741935798\n",
            "Score:  18.86626503979139\n",
            "Score:  25.816922979091057\n",
            "Score:  38.78432186915761\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 58.4         |\n",
            "|    ep_rew_mean          | 33.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 12           |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 250          |\n",
            "|    total_timesteps      | 3072         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076240995 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.673       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.003        |\n",
            "|    loss                 | 57.6         |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    value_loss           | 94.3         |\n",
            "------------------------------------------\n",
            "Score:  22.93876086141814\n",
            "Score:  41.02948242284413\n",
            "Score:  15.956305253840362\n",
            "Score:  15.270079185312122\n",
            "Score:  35.229135431963606\n",
            "Score:  34.22304460770777\n",
            "Score:  16.558962755694672\n",
            "Score:  21.444296128149613\n",
            "Score:  16.026289850950732\n",
            "Score:  77.7266764799695\n",
            "Score:  25.81460096878264\n",
            "Score:  45.69910884906208\n",
            "Score:  34.19818444102328\n",
            "Score:  18.462023535718316\n",
            "Score:  40.93268441264055\n",
            "Score:  27.836797640211152\n",
            "Score:  26.910009002988875\n",
            "Score:  53.574201874506024\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 58.5        |\n",
            "|    ep_rew_mean          | 32.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 11          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 359         |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007592085 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.673      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.003       |\n",
            "|    loss                 | 20.6        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.000884   |\n",
            "|    value_loss           | 69.5        |\n",
            "-----------------------------------------\n",
            "Score:  20.177701700329003\n",
            "Score:  37.30688477121113\n",
            "Score:  16.779918602786868\n",
            "Score:  23.35596425074291\n",
            "Score:  30.396601532866875\n",
            "Score:  76.54858303896135\n",
            "Score:  47.29823376091386\n",
            "Score:  15.382829074871067\n",
            "Score:  19.710691601671662\n",
            "Score:  37.54481550937633\n",
            "Score:  17.711270193158438\n",
            "Score:  22.694581477772843\n",
            "Score:  18.000152483492734\n",
            "Score:  26.232838232009698\n",
            "Score:  18.96188879450933\n",
            "Score:  23.306861991681384\n",
            "Score:  20.4382958416262\n",
            "Score:  70.03926271325881\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 58.2         |\n",
            "|    ep_rew_mean          | 32.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 10           |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 468          |\n",
            "|    total_timesteps      | 5120         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003637275 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.684       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.003        |\n",
            "|    loss                 | 52.4         |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | 0.000125     |\n",
            "|    value_loss           | 76.1         |\n",
            "------------------------------------------\n",
            "Score:  23.849470662510864\n",
            "Score:  21.917069051108776\n",
            "Score:  17.969408575554212\n",
            "Score:  35.164451129249166\n",
            "Score:  37.79027445758461\n",
            "Score:  18.51772630027088\n",
            "Score:  37.5932416441963\n",
            "Score:  129.5711323349611\n",
            "Score:  20.165075270900928\n",
            "Score:  17.264597749117783\n",
            "Score:  33.64661631747555\n",
            "Score:  17.74854872391751\n",
            "Score:  20.55397894225067\n",
            "Score:  15.988135641651823\n",
            "Score:  19.136252734798187\n",
            "Score:  17.709737258872238\n",
            "Score:  70.65996332899408\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 57.8         |\n",
            "|    ep_rew_mean          | 31.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 10           |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 577          |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020032013 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.688       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.003        |\n",
            "|    loss                 | 39.6         |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.000302    |\n",
            "|    value_loss           | 76.2         |\n",
            "------------------------------------------\n",
            "Score:  31.71746635671507\n",
            "Score:  43.18949688332612\n",
            "Score:  74.22413894235935\n",
            "Score:  42.799506202369116\n",
            "Score:  32.5412894670106\n",
            "Score:  25.0601007528227\n",
            "Score:  13.168171155024869\n",
            "Score:  18.260202144284378\n",
            "Score:  71.86288557178706\n",
            "Score:  20.398770648672045\n",
            "Score:  37.321132120837675\n",
            "Score:  20.916582239523343\n",
            "Score:  15.156515852946917\n",
            "Score:  37.33965595961097\n",
            "Score:  27.775586222899108\n",
            "Score:  26.140585645357444\n",
            "Score:  22.02145449713479\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 58.4        |\n",
            "|    ep_rew_mean          | 32          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 10          |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 687         |\n",
            "|    total_timesteps      | 7168        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009414646 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.681      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.003       |\n",
            "|    loss                 | 35.8        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00177    |\n",
            "|    value_loss           | 93.3        |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/balancing_ball/ppo_balancing_ball_final.zip /content/abc.zip"
      ],
      "metadata": {
        "id": "vsVOGuaRjlZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ppo(\n",
        "  total_timesteps=10000,\n",
        "  n_steps=1024,\n",
        "  batch_size=32,\n",
        "  difficulty=\"medium\",\n",
        "  n_envs=1,\n",
        "  load_model=\"/content/models/ppo_balancing_ball_final.zip\",\n",
        "  eval_episodes=2,\n",
        ")"
      ],
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}