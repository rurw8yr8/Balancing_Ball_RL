{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81",
        "SJR1k4u6y9FH",
        "90pbC8lLWB04"
      ],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame-ce pymunk stable-baselines3 shimmy>=2.0"
      ],
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "d4f2a5dc-aa04-4ef4-d29f-f4018987bf97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'=2.0'\t best_model1.zip   capture   game_history   logs   models   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ],
      "metadata": {
        "id": "6bbqyvjZ1PZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recorder"
      ],
      "metadata": {
        "id": "hhEqO-xFu4AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "class Recorder:\n",
        "\n",
        "    def __init__(self, task: str = \"game_history_record\"):\n",
        "        \"\"\"\n",
        "        tasks:\n",
        "        1. game_history_record\n",
        "        2. temp_memory\n",
        "        \"\"\"\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \"\"\n",
        "        if task == \"game_history_record\":\n",
        "            collection_name = self.get_newest_record_name()\n",
        "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.json_file_path):\n",
        "            print(\"Loading the json memory file\")\n",
        "            self.memory = self.load(self.json_file_path)\n",
        "        else:\n",
        "            print(\"The json memory file does not exist. Creating new file.\")\n",
        "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
        "            with open(self.json_file_path, \"w\") as f:\n",
        "                json.dump(self.memory, f)\n",
        "\n",
        "    def get(self):\n",
        "        print(\"Getting the json memory\")\n",
        "        return self.memory\n",
        "\n",
        "    def add_no_limit(self, data: float, ):\n",
        "        \"\"\"\n",
        "        Add a records.\n",
        "\n",
        "        Args:\n",
        "            role: The role of the sender (e.g., 'user', 'assistant')\n",
        "            message: The message content\n",
        "        \"\"\"\n",
        "        self.memory[\"game_records\"].append({\n",
        "            \"game_total_duration\": data,\n",
        "            \"timestamp\": str(datetime.datetime.now())\n",
        "        })\n",
        "\n",
        "        self.save(self.json_file_path)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(self.memory, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving memory to {file_path}: {e}\")\n",
        "\n",
        "    def load(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading memory from {file_path}: {e}\")\n",
        "            return {\"game_records\": []}\n",
        "\n",
        "    def get_newest_record_name(self) -> str:\n",
        "        \"\"\"\n",
        "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
        "            - 例如: \"game_2022-01\"\n",
        "        \"\"\"\n",
        "\n",
        "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
        "        return \"record_\" + this_month"
      ],
      "metadata": {
        "id": "gJwfQb_Yuz1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shapes & Objects"
      ],
      "metadata": {
        "id": "cnA8wZtosmeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Shape:\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape: Optional[pymunk.Shape] = None,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a physical shape with associated body.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the body\n",
        "            velocity: Initial velocity (vx, vy) of the body\n",
        "            body: The pymunk Body to attach to this shape\n",
        "            shape: The pymunk Shape for collision detection\n",
        "        \"\"\"\n",
        "\n",
        "        self.body = body\n",
        "        self.default_position = position\n",
        "        self.default_velocity = velocity\n",
        "        self.body.position = position\n",
        "        self.body.velocity = velocity\n",
        "        self.default_angular_velocity = 0\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
        "        self.body.position = self.default_position\n",
        "        self.body.velocity = self.default_velocity\n",
        "        self.body.angular_velocity = self.default_angular_velocity\n"
      ],
      "metadata": {
        "id": "5wMhHMWCsmVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "\n",
        "# from shapes.shape import Shape\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Circle(Shape):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape_radio: float = 20,\n",
        "                shape_mass: float = 1,\n",
        "                shape_friction: float = 0.1,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a circular physics object.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the circle\n",
        "            velocity: Initial velocity (vx, vy) of the circle\n",
        "            body: The pymunk Body to attach this circle to\n",
        "            shape_radio: Radius of the circle in pixels\n",
        "            shape_mass: Mass of the circle\n",
        "            shape_friction: Friction coefficient for the circle\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(position, velocity, body)\n",
        "        self.shape_radio = shape_radio\n",
        "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
        "        self.shape.mass = shape_mass\n",
        "        self.shape.friction = shape_friction\n",
        "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
      ],
      "metadata": {
        "id": "mfw5tBxBswyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game class"
      ],
      "metadata": {
        "id": "v-8d5fKltI62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "import pygame\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import display, Image, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import IPython.display as ipd\n",
        "# from shapes.circle import Circle\n",
        "# from record import Recorder\n",
        "\n",
        "class BalancingBallGame:\n",
        "    \"\"\"\n",
        "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Game constants\n",
        "\n",
        "\n",
        "    # Visual settings for indie style\n",
        "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
        "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
        "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
        "    PARTICLE_COLORS = [(252, 186, 3), (252, 127, 3), (252, 3, 3)]  # Fire-like particles\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode: str = \"human\",\n",
        "                 sound_enabled: bool = True,\n",
        "                 difficulty: str = \"medium\",\n",
        "                 window_x: int = 1000,\n",
        "                 window_y: int = 600,\n",
        "                 max_step: int = 30000,\n",
        "                 reward_staying_alive: float = 0.1,\n",
        "                 reward_ball_centered: float = 0.2,\n",
        "                 penalty_falling: float = -10.0,\n",
        "                 fps: int = 120,\n",
        "                 platform_shape: str = \"circle\",\n",
        "                 platform_length: int = 200,\n",
        "                 capture_per_second: int = None,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialize the balancing ball game.\n",
        "\n",
        "        Args:\n",
        "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
        "            sound_enabled: Whether to enable sound effects\n",
        "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
        "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
        "            reward_staying_alive: float = 0.1,\n",
        "            reward_ball_centered: float = 0.2,\n",
        "            penalty_falling: float = -10.0,\n",
        "            fps: frame per second\n",
        "            capture_per_second: save game screen as a image every second, None means no capture\n",
        "        \"\"\"\n",
        "        # Game parameters\n",
        "        self.max_step = max_step\n",
        "        self.reward_staying_alive = reward_staying_alive\n",
        "        self.reward_ball_centered = reward_ball_centered\n",
        "        self.penalty_falling = penalty_falling\n",
        "        self.fps = fps\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "        self.recorder = Recorder(\"game_history_record\")\n",
        "        self.render_mode = render_mode\n",
        "        self.sound_enabled = sound_enabled\n",
        "        self.difficulty = difficulty\n",
        "\n",
        "        self._get_x_axis_max_reward_rate(platform_length)\n",
        "\n",
        "        # Initialize physics space\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = (0, 1000)\n",
        "        self.space.damping = 0.9\n",
        "\n",
        "        # Create game bodies\n",
        "        self.dynamic_body = pymunk.Body()  # Ball body\n",
        "        self.kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        self.kinematic_body.position = (self.window_x / 2, 400)\n",
        "        self.default_kinematic_position = self.kinematic_body.position\n",
        "\n",
        "        # Create game objects\n",
        "        self._create_ball()\n",
        "        self._create_platform(platform_shape=platform_shape, platform_length=platform_length)\n",
        "        # self._create_platform(\"rectangle\")\n",
        "\n",
        "        # Add all objects to space\n",
        "        self.space.add(self.dynamic_body, self.kinematic_body,\n",
        "                       self.circle.shape, self.platform)\n",
        "\n",
        "        # Game state tracking\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Initialize Pygame if needed\n",
        "        if self.render_mode in [\"human\", \"rgb_array\", \"rgb_array_and_human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            self._setup_pygame()\n",
        "        else:\n",
        "            print(\"render_mode is not human or rgb_array, so no pygame setup.\")\n",
        "\n",
        "        # Set difficulty parameters\n",
        "        self._apply_difficulty()\n",
        "        self.capture_per_second = capture_per_second\n",
        "\n",
        "        # Create folders for captures if needed\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \".\"\n",
        "        os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
        "\n",
        "    def _setup_pygame(self):\n",
        "        \"\"\"Set up PyGame for rendering\"\"\"\n",
        "        pygame.init()\n",
        "        self.frame_count = 0\n",
        "\n",
        "        if self.sound_enabled:\n",
        "            self._load_sounds()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
        "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\": # todo\n",
        "            from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))  # Create hidden surface\n",
        "\n",
        "            # Set up display in Colab\n",
        "            self.draw_options = DrawOptions(self.screen)\n",
        "            html_display = ipd.HTML('''\n",
        "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
        "                </div>\n",
        "            ''')\n",
        "            self.display_handle = display(html_display, display_id='pygame_display')\n",
        "\n",
        "            self.last_update_time = time.time()\n",
        "            self.update_interval = 1.0 / 15  # Update display at 15 FPS to avoid overwhelming Colab\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid render mode. Using headless mode.\")\n",
        "\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.font = pygame.font.Font(None, 30)\n",
        "\n",
        "        # Create custom draw options for indie style\n",
        "\n",
        "    def _load_sounds(self):\n",
        "        \"\"\"Load game sound effects\"\"\"\n",
        "        try:\n",
        "            pygame.mixer.init()\n",
        "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
        "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
        "        except Exception:\n",
        "            print(\"Sound loading error\")\n",
        "            self.sound_enabled = False\n",
        "            pass\n",
        "\n",
        "    def _create_ball(self):\n",
        "        \"\"\"Create the ball with physics properties\"\"\"\n",
        "        self.ball_radius = 15\n",
        "        self.circle = Circle(\n",
        "            position=(self.window_x / 2, 200),\n",
        "            velocity=(0, 0),\n",
        "            body=self.dynamic_body,\n",
        "            shape_radio=self.ball_radius,\n",
        "            shape_friction=100,\n",
        "        )\n",
        "        # Store initial values for reset\n",
        "        self.default_ball_position = self.dynamic_body.position\n",
        "\n",
        "    def _create_platform(self,\n",
        "                         platform_shape: str = \"circle\",\n",
        "                         platform_length: int = 200\n",
        "                        ):\n",
        "        \"\"\"\n",
        "        Create the platform with physics properties\n",
        "        platform_shape: circle, rectangle\n",
        "        platform_length: Length of a rectangle or Diameter of a circle\n",
        "        \"\"\"\n",
        "        if platform_shape == \"circle\":\n",
        "            self.platform_length = platform_length / 2 # radius\n",
        "            self.platform = pymunk.Circle(self.kinematic_body, self.platform_length)\n",
        "            self.platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
        "            self.platform.friction = 0.7\n",
        "        elif platform_shape == \"rectangle\":\n",
        "            self.platform_length = 200\n",
        "            vs = [(-self.platform_length/2, -10),\n",
        "                (self.platform_length/2, -10),\n",
        "                (self.platform_length/2, 10),\n",
        "                (-self.platform_length/2, 10)]\n",
        "\n",
        "            self.platform = pymunk.Poly(self.kinematic_body, vs)\n",
        "        self.platform.friction = 0.7\n",
        "        self.platform_rotation = 0\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "    def _apply_difficulty(self):\n",
        "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
        "        if self.difficulty == \"easy\":\n",
        "            self.max_platform_speed = 1.5\n",
        "            self.ball_elasticity = 0.5\n",
        "        elif self.difficulty == \"medium\":\n",
        "            self.max_platform_speed = 2.5\n",
        "            self.ball_elasticity = 0.7\n",
        "        else:  # hard\n",
        "            self.max_platform_speed = 3.5\n",
        "            self.ball_elasticity = 0.9\n",
        "\n",
        "        self.circle.shape.elasticity = self.ball_elasticity\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
        "        # Reset physics objects\n",
        "        self.dynamic_body.position = self.default_ball_position\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "\n",
        "        self.kinematic_body.position = self.default_kinematic_position\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        # Reset game state\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Return initial observation\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Take a step in the game using the given action.\n",
        "\n",
        "        Args:\n",
        "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
        "\n",
        "        Returns:\n",
        "            observation: Game state observation\n",
        "            reward: Reward for this step\n",
        "            terminated: Whether episode is done\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # Apply action to platform rotation\n",
        "        self.dynamic_body.angular_velocity += action\n",
        "\n",
        "        # Step the physics simulation\n",
        "        self.space.step(1/self.fps)\n",
        "\n",
        "        # Update particle effects\n",
        "        self._update_particles()\n",
        "\n",
        "        # Check game state\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "        reward = self.reward_staying_alive\n",
        "\n",
        "        # Calculate reward for keeping ball centered on platform\n",
        "        ball_x = self.dynamic_body.position[0]\n",
        "\n",
        "        # Check if ball falls off screen\n",
        "        if (self.dynamic_body.position[1] > self.kinematic_body.position[1] or\n",
        "            self.dynamic_body.position[0] < 0 or\n",
        "            self.dynamic_body.position[0] > self.window_x or\n",
        "            self.steps >= self.max_step\n",
        "            ):\n",
        "\n",
        "            print(\"Score: \", self.score)\n",
        "            terminated = True\n",
        "            reward = self.penalty_falling if self.steps < self.max_step else 0\n",
        "            self.game_over = True\n",
        "\n",
        "            result = {\n",
        "                \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
        "                \"score\": self.score,\n",
        "            }\n",
        "            self.recorder.add_no_limit(result)\n",
        "\n",
        "            if self.sound_enabled and self.sound_fall:\n",
        "                self.sound_fall.play()\n",
        "\n",
        "        step_reward = self._reward_calculator(ball_x)\n",
        "        self.score += step_reward\n",
        "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
        "        return self._get_observation(), step_reward, terminated\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
        "        # update particles and draw them\n",
        "        self.render()\n",
        "\n",
        "        if self.render_mode == \"rgb_array_and_human_in_colab\":\n",
        "            # Update the image in Colab output at a reasonable interval\n",
        "            current_time = time.time()\n",
        "            if current_time - self.last_update_time >= self.update_interval:\n",
        "                # Convert Pygame surface to an image that can be displayed in Colab\n",
        "                buffer = BytesIO()\n",
        "                pygame.image.save(self.screen, buffer, 'PNG')\n",
        "                buffer.seek(0)\n",
        "                img_data = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "\n",
        "                # Update the HTML image\n",
        "                self.display_handle.update(ipd.HTML(f'''\n",
        "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                        <img id=\"pygame-img\" src=\"data:image/png;base64,{img_data}\" style=\"width:100%;\">\n",
        "                    </div>\n",
        "                '''))\n",
        "\n",
        "                self.last_update_time = current_time\n",
        "\n",
        "        if self.capture_per_second is not None and self.frame_count % self.capture_per_second == 0:  # Every second at 60 FPS\n",
        "            pygame.image.save(self.screen, f\"capture/frame_{self.frame_count/60}.png\")\n",
        "\n",
        "        self.frame_count += 1\n",
        "        screen_data = pygame.surfarray.array3d(self.screen)  # 获取数据\n",
        "        screen_data = np.transpose(screen_data, (1, 0, 2))  # 转置以符合 (height, width, channels)\n",
        "\n",
        "        return screen_data\n",
        "\n",
        "    def _update_particles(self):\n",
        "        \"\"\"Update particle effects for indie visual style\"\"\"\n",
        "        # Create new particles when ball hits platform\n",
        "        if abs(self.dynamic_body.position[1] - (self.kinematic_body.position[1] - 20)) < 5 and abs(self.dynamic_body.velocity[1]) > 100:\n",
        "            for _ in range(5):\n",
        "                self.particles.append({\n",
        "                    'x': self.dynamic_body.position[0],\n",
        "                    'y': self.dynamic_body.position[1] + self.ball_radius,\n",
        "                    'vx': random.uniform(-2, 2),\n",
        "                    'vy': random.uniform(1, 3),\n",
        "                    'life': 30,\n",
        "                    'size': random.uniform(2, 5),\n",
        "                    'color': random.choice(self.PARTICLE_COLORS)\n",
        "                })\n",
        "\n",
        "            if self.sound_enabled and self.sound_bounce:\n",
        "                self.sound_bounce.play()\n",
        "\n",
        "        # Update existing particles\n",
        "        for particle in self.particles[:]:\n",
        "            particle['x'] += particle['vx']\n",
        "            particle['y'] += particle['vy']\n",
        "            particle['life'] -= 1\n",
        "            if particle['life'] <= 0:\n",
        "                self.particles.remove(particle)\n",
        "\n",
        "    def render(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Render the current game state\"\"\"\n",
        "        if self.render_mode == \"headless\":\n",
        "            return None\n",
        "\n",
        "        # Clear screen with background color\n",
        "        self.screen.fill(self.BACKGROUND_COLOR)\n",
        "\n",
        "        # Custom drawing (for indie style)\n",
        "        self._draw_indie_style()\n",
        "\n",
        "        # Draw game information\n",
        "        self._draw_game_info()\n",
        "\n",
        "        # Update display if in human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.fps)\n",
        "            return None\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            # Return RGB array for gym environment\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\":\n",
        "            self.space.debug_draw(self.draw_options)\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        # platform_points = []\n",
        "        # for v in self.platform.get_vertices():\n",
        "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
        "        #     platform_points.append((int(x), int(y)))\n",
        "\n",
        "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "\n",
        "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "        # Draw rotation direction indicator\n",
        "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
        "\n",
        "        # Draw ball with gradient and glow\n",
        "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "        # Draw particles\n",
        "        for particle in self.particles:\n",
        "            alpha = min(255, int(255 * (particle['life'] / 30)))\n",
        "            pygame.draw.circle(\n",
        "                self.screen,\n",
        "                particle['color'],\n",
        "                (int(particle['x']), int(particle['y'])),\n",
        "                int(particle['size'])\n",
        "            )\n",
        "\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = self.kinematic_body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
        "\n",
        "    def _draw_game_info(self):\n",
        "        \"\"\"Draw game information on screen\"\"\"\n",
        "        # Create texts\n",
        "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
        "        score_text = f\"Score: {self.score}\"\n",
        "\n",
        "        # Render texts\n",
        "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
        "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
        "\n",
        "        # Draw text backgrounds\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
        "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
        "\n",
        "        # Draw texts\n",
        "        self.screen.blit(time_surface, (10, 10))\n",
        "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
        "\n",
        "        # Draw game over screen\n",
        "        if self.game_over:\n",
        "            game_over_text = \"GAME OVER - Press R to restart\"\n",
        "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw semi-transparent background\n",
        "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
        "            overlay.fill((0, 0, 0, 128))\n",
        "            self.screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Draw text\n",
        "            self.screen.blit(game_over_surface,\n",
        "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
        "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
        "\n",
        "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
        "        \"\"\"\n",
        "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
        "        center of game window coordinates. The closer you are, the higher the reward.\n",
        "\n",
        "        When the ball is to be 10 points away from the center coordinates,\n",
        "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        \"\"\"\n",
        "        self.reward_width = (platform_length / 2) - 5\n",
        "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
        "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
        "\n",
        "    def _reward_calculator(self, ball_x):\n",
        "        # score & reward\n",
        "        if self.steps < 2000:\n",
        "            step_reward = self.steps * 0.01\n",
        "        elif self.steps < 5000:\n",
        "            step_reward = self.steps * 0.03\n",
        "        else:\n",
        "            step_reward = self.steps * 0.05\n",
        "\n",
        "        rw = abs(ball_x - self.window_x/2)\n",
        "        if rw < self.reward_width:\n",
        "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "            return step_reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the game and clean up resources\"\"\"\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            pygame.quit()\n",
        "\n",
        "    def run_standalone(self):\n",
        "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
        "        # if self.render_mode not in [\"human\"]:\n",
        "        #     raise ValueError(\"Standalone mode requires render_mode='human'\")\n",
        "\n",
        "        running = True\n",
        "        while running:\n",
        "            # Handle events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_r and self.game_over:\n",
        "                        self.reset()\n",
        "\n",
        "            # Process keyboard controls\n",
        "            keys = pygame.key.get_pressed()\n",
        "            action = 0\n",
        "            if keys[pygame.K_LEFT]:\n",
        "                action = -1.0\n",
        "            if keys[pygame.K_RIGHT]:\n",
        "                action = 1.0\n",
        "\n",
        "            # Take game step\n",
        "            if not self.game_over:\n",
        "                self.step(action)\n",
        "\n",
        "            # Render\n",
        "            self.render()\n",
        "\n",
        "        self.close()"
      ],
      "metadata": {
        "id": "-wiw5Rjks-xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42e6ee4-1a9b-4448-e6db-1560f2081407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame-ce 2.5.3 (SDL 2.30.12, Python 3.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GYM env"
      ],
      "metadata": {
        "id": "gjobL-nozI81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "class BalancingBallEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for the Balancing Ball game\n",
        "    \"\"\"\n",
        "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
        "\n",
        "    def __init__(self, render_mode=\"rgb_array\", difficulty=\"medium\", fps=30):\n",
        "        super(BalancingBallEnv, self).__init__()\n",
        "\n",
        "        # Action space: discrete - 0: left, 1: right\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Initialize game\n",
        "        self.window_x = 1000\n",
        "        self.window_y = 600\n",
        "        self.platform_shape = \"circle\"\n",
        "        self.platform_length = 200\n",
        "\n",
        "        self.stack_size = 3  # Number of frames to stack\n",
        "        self.observation_stack = []  # Initialize the stack\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.game = BalancingBallGame(\n",
        "            render_mode=render_mode,\n",
        "            sound_enabled=(render_mode == \"human\"),\n",
        "            difficulty=difficulty,\n",
        "            window_x = self.window_x,\n",
        "            window_y = self.window_y,\n",
        "            platform_shape = self.platform_shape,\n",
        "            platform_length = self.platform_length,\n",
        "            fps = fps,\n",
        "        )\n",
        "\n",
        "        # Image observation space (RGB) with stacked frames\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255,\n",
        "            shape=(self.window_y, self.window_x, 3 * self.stack_size),  # For stacked frames\n",
        "            dtype=np.uint8\n",
        "        )\n",
        "\n",
        "        # Platform_length /= 2 when for calculate the distance to the\n",
        "        # center of game window coordinates. The closer you are, the higher the reward.\n",
        "        self.platform_length = (self.platform_length / 2) - 5\n",
        "\n",
        "        # When the ball is to be 10 points away from the center coordinates,\n",
        "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        self.x_axis_max_reward_rate = 0.5 / self.platform_length\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Convert from discrete action to the game's expected format\n",
        "        action_value = -1.0 if action == 0 else 1.0\n",
        "\n",
        "        # Take step in the game\n",
        "        # todo\n",
        "        # 修改代码变成模型执行一次动作然后在接下来的一定禎数持续该动作，同时收集并且堆叠祯然后给模型预测下一次动作\n",
        "        # 比如一次循环为6祯，那麼模型一次动作将持续六祯，同时堆叠该6祯给模型预测下一次动作\n",
        "        obs, step_reward, terminated = self.game.step(action_value)\n",
        "\n",
        "        # Stack the frames\n",
        "        self.observation_stack.append(obs)\n",
        "        if len(self.observation_stack) > self.stack_size:\n",
        "            self.observation_stack.pop(0)  # Remove the oldest frame\n",
        "\n",
        "        # If the stack isn't full yet, pad it with the current frame\n",
        "        while len(self.observation_stack) < self.stack_size:\n",
        "            self.observation_stack.insert(0, obs)  # Pad with current frame at the beginning\n",
        "\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return stacked_obs, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        observation = self.game.reset()\n",
        "\n",
        "        # Reset the observation stack\n",
        "        self.observation_stack = []\n",
        "\n",
        "        # Fill the stack with the initial observation\n",
        "        for _ in range(self.stack_size):\n",
        "            self.observation_stack.append(observation)\n",
        "\n",
        "        # Create stacked observation\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        info = {}\n",
        "        return stacked_obs, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        return self.game.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.game.close()"
      ],
      "metadata": {
        "id": "MBzvHTN1zJu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "SJR1k4u6y9FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "\n",
        "# # from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "# def run_standalone_game(difficulty=\"medium\"):\n",
        "#     \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "#     window_x = 1000\n",
        "#     window_y = 600\n",
        "#     platform_shape = \"circle\"\n",
        "#     platform_length = 200\n",
        "\n",
        "#     game = BalancingBallGame(\n",
        "#         difficulty=difficulty,\n",
        "#         window_x = window_x,\n",
        "#         window_y = window_y,\n",
        "#         platform_shape = platform_shape,\n",
        "#         platform_length = platform_length,\n",
        "#         fps = 30,\n",
        "#     )\n",
        "\n",
        "#     game.run_standalone()\n",
        "\n",
        "# def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "#     \"\"\"Test the OpenAI Gym environment\"\"\"\n",
        "#     import time\n",
        "#     # from gym_env import BalancingBallEnv\n",
        "\n",
        "#     fps = 30\n",
        "#     env = BalancingBallEnv(\n",
        "#         render_mode=\"human\",\n",
        "#         difficulty=difficulty,\n",
        "#         fps=fps,\n",
        "#     )\n",
        "\n",
        "#     for episode in range(episodes):\n",
        "#         observation, info = env.reset()\n",
        "#         total_reward = 0\n",
        "#         step = 0\n",
        "#         done = False\n",
        "\n",
        "#         while not done:\n",
        "#             # Sample a random action (for testing only)\n",
        "#             action = env.action_space.sample()\n",
        "\n",
        "#             # Take step\n",
        "#             observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "#             done = terminated or truncated\n",
        "#             total_reward += reward\n",
        "#             step += 1\n",
        "\n",
        "#             # Render\n",
        "#             env.render()\n",
        "\n",
        "#         print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "#     env.close()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # parser = argparse.ArgumentParser(description='Run Balancing Ball game')\n",
        "#     # parser.add_argument('--mode', type=str, default='gym',\n",
        "#     #                     choices=['game', 'gym'],\n",
        "#     #                     help='Mode to run: standalone game or gym environment test')\n",
        "#     # parser.add_argument('--difficulty', type=str, default='medium',\n",
        "#     #                     choices=['easy', 'medium', 'hard'],\n",
        "#     #                     help='Game difficulty level')\n",
        "#     # args = parser.parse_args()\n",
        "\n",
        "#     # if args.mode == 'game':\n",
        "#     #     run_standalone_game(difficulty=args.difficulty)\n",
        "#     # else:\n",
        "#     #     test_gym_env(difficulty=args.difficulty)\n",
        "#     # run_standalone_game(difficulty=\"medium\")\n",
        "#     test_gym_env(difficulty=\"medium\")"
      ],
      "metadata": {
        "id": "Uea1POIGtIff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train func"
      ],
      "metadata": {
        "id": "90pbC8lLWB04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticCnnPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Add the game directory to the system path\n",
        "# sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"game_base_files_test\"))\n",
        "\n",
        "# from gym_env import BalancingBallEnv\n",
        "\n",
        "# support render_mode: human, rgb_array, rgb_array_and_human, rgb_array_and_human_in_colab\n",
        "def make_env(render_mode=\"rgb_array\", difficulty=\"medium\"):\n",
        "    \"\"\"\n",
        "    Create and return an environment function to be used with VecEnv\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def train_ppo(\n",
        "    total_timesteps=1000000,\n",
        "    learning_rate=0.0003,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    ent_coef=0.01,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    policy_kwargs=None,\n",
        "    n_envs=4,\n",
        "    save_freq=10000,\n",
        "    log_dir=\"./logs/\",\n",
        "    model_dir=\"./models/\",\n",
        "    eval_freq=10000,\n",
        "    eval_episodes=5,\n",
        "    difficulty=\"medium\",\n",
        "    load_model=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "    Args:\n",
        "        total_timesteps: Total number of steps to train for\n",
        "        n_envs: Number of parallel environments\n",
        "        save_freq: How often to save checkpoints (in timesteps)\n",
        "        log_dir: Directory for tensorboard logs\n",
        "        model_dir: Directory to save models\n",
        "        eval_freq: How often to evaluate the model (in timesteps)\n",
        "        eval_episodes: Number of episodes to evaluate on\n",
        "        difficulty: Game difficulty level\n",
        "        load_model: Path to model to load for continued training\n",
        "    \"\"\"\n",
        "    # Create directories\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Setup environments\n",
        "    # support render_mode: human, rgb_array, rgb_array_and_human, rgb_array_and_human_in_colab\n",
        "    env = make_vec_env(\n",
        "        make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
        "        n_envs=n_envs\n",
        "    )\n",
        "\n",
        "    # Apply VecTransposeImage to correctly handle image observations\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    # Setup evaluation environment\n",
        "    eval_env = make_vec_env(\n",
        "        make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
        "        n_envs=1\n",
        "    )\n",
        "    eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "    # Define policy kwargs if not provided\n",
        "    if policy_kwargs is None:\n",
        "        policy_kwargs = {\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "        }\n",
        "\n",
        "    # Create the PPO model\n",
        "    if load_model:\n",
        "        print(f\"Loading model from {load_model}\")\n",
        "        model = PPO.load(\n",
        "            load_model,\n",
        "            env=env,\n",
        "            tensorboard_log=log_dir,\n",
        "        )\n",
        "    else:\n",
        "        model = PPO(\n",
        "            policy=ActorCriticCnnPolicy,\n",
        "            env=env,\n",
        "            learning_rate=learning_rate,\n",
        "            n_steps=n_steps,\n",
        "            batch_size=batch_size,\n",
        "            n_epochs=n_epochs,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            ent_coef=ent_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            tensorboard_log=log_dir,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "    # Setup callbacks\n",
        "    checkpoint_callback = CheckpointCallback(\n",
        "        save_freq=save_freq // n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "        save_path=model_dir,\n",
        "        name_prefix=\"ppo_balancing_ball\"\n",
        "    )\n",
        "\n",
        "    eval_callback = EvalCallback(\n",
        "        eval_env,\n",
        "        best_model_save_path=model_dir,\n",
        "        log_path=log_dir,\n",
        "        eval_freq=eval_freq // n_envs,\n",
        "        n_eval_episodes=eval_episodes,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[checkpoint_callback, eval_callback],\n",
        "    )\n",
        "\n",
        "    # Save the final model\n",
        "    model.save(f\"{model_dir}/ppo_balancing_ball_final\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n",
        "\n",
        "def evaluate(model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model\n",
        "        n_episodes: Number of episodes to evaluate on\n",
        "        difficulty: Game difficulty level\n",
        "    \"\"\"\n",
        "    # Create environment for evaluation\n",
        "    env = make_vec_env(\n",
        "        make_env(render_mode=\"human\", difficulty=difficulty),\n",
        "        n_envs=1\n",
        "    )\n",
        "    env = VecTransposeImage(env)\n",
        "\n",
        "    # Load the model\n",
        "    model = PPO.load(model_path)\n",
        "\n",
        "    # Evaluate\n",
        "    mean_reward, std_reward = evaluate_policy(\n",
        "        model,\n",
        "        env,\n",
        "        n_eval_episodes=n_episodes,\n",
        "        deterministic=True,\n",
        "        render=True\n",
        "    )\n",
        "\n",
        "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "  # import argparse\n",
        "\n",
        "  # parser = argparse.ArgumentParser(description=\"Train or evaluate PPO agent for Balancing Ball\")\n",
        "  # parser.add_argument(\"--mode\", type=str, default=\"train\", choices=[\"train\", \"eval\"],\n",
        "  #                     help=\"Mode: 'train' to train model, 'eval' to evaluate\")\n",
        "  # parser.add_argument(\"--timesteps\", type=int, default=1000000,\n",
        "  #                     help=\"Total timesteps for training\")\n",
        "  # parser.add_argument(\"--difficulty\", type=str, default=\"medium\",\n",
        "  #                     choices=[\"easy\", \"medium\", \"hard\"],\n",
        "  #                     help=\"Game difficulty\")\n",
        "  # parser.add_argument(\"--load_model\", type=str, default=None,\n",
        "  #                     help=\"Path to model to load for continued training or evaluation\")\n",
        "  # parser.add_argument(\"--n_envs\", type=int, default=4,\n",
        "  #                     help=\"Number of parallel environments for training\")\n",
        "  # parser.add_argument(\"--eval_episodes\", type=int, default=5,\n",
        "  #                     help=\"Number of episodes for evaluation\")\n",
        "\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "  # if args.mode == \"train\":\n",
        "  #     train_ppo(\n",
        "  #         total_timesteps=args.timesteps,\n",
        "  #         difficulty=args.difficulty,\n",
        "  #         n_envs=args.n_envs,\n",
        "  #         load_model=args.load_model,\n",
        "  #         eval_episodes=args.eval_episodes,\n",
        "  #     )\n",
        "  # else:\n",
        "  #     if args.load_model is None:\n",
        "  #         print(\"Error: Must provide --load_model for evaluation\")\n",
        "  #     else:\n",
        "  #         evaluate(\n",
        "  #             model_path=args.load_model,\n",
        "  #             n_episodes=args.eval_episodes,\n",
        "  #             difficulty=args.difficulty\n",
        "  #         )"
      ],
      "metadata": {
        "id": "VTdFslcyWAT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "aFM-k9MuCmzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ppo(\n",
        "#   total_timesteps=20000,\n",
        "#   n_steps=1024,\n",
        "#   batch_size=32,\n",
        "#   difficulty=\"medium\",\n",
        "#   n_envs=1,\n",
        "#   load_model=None,\n",
        "#   eval_episodes=2,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "Ndr9hGp2CZzF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqNqMgn7nO5c",
        "outputId": "fdf5f468-24d2-40cf-8c3e-0ab36163e055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/models/best_model.zip /content/best_model1.zip"
      ],
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ppo(\n",
        "  total_timesteps=20000,\n",
        "  learning_rate=0.0001,\n",
        "  n_steps=1024,\n",
        "  batch_size=32,\n",
        "  difficulty=\"medium\",\n",
        "  n_envs=1,\n",
        "  load_model=\"/content/models/best_model.zip\",\n",
        "  eval_episodes=2,\n",
        ")\n",
        "# train_ppo(\n",
        "#   total_timesteps=10000,\n",
        "#   n_steps=1024,\n",
        "#   batch_size=32,\n",
        "#   difficulty=\"medium\",\n",
        "#   n_envs=1,\n",
        "#   load_model=\"/content/models/ppo_balancing_ball_final.zip\",\n",
        "#   eval_episodes=2,\n",
        "# )"
      ],
      "metadata": {
        "id": "tLCe2GS6Kb8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656500d1-2a83-460f-c6f1-a47e6ad49032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The json memory file does not exist. Creating new file.\n",
            "self.x_axis_max_reward_rate:  0.021052631578947368\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.021052631578947368\n",
            "Loading model from /content/models/best_model.zip\n",
            "Starting training...\n",
            "Logging to ./logs/PPO_1\n",
            "Score:  19.77267581964471\n",
            "Score:  214.26620076675337\n",
            "Score:  19.11288600052692\n",
            "Score:  189.37195833436888\n",
            "Score:  18.511079096843883\n",
            "Score:  82.94929655298317\n",
            "Score:  188.35519638438987\n",
            "Score:  538.2727084817814\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 106      |\n",
            "|    ep_rew_mean     | 159      |\n",
            "| time/              |          |\n",
            "|    fps             | 16       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 61       |\n",
            "|    total_timesteps | 1024     |\n",
            "---------------------------------\n",
            "Score:  739.5974367746129\n",
            "Score:  33.04901388575799\n",
            "Score:  28.66704614951879\n",
            "Score:  31.403136305975558\n",
            "Score:  17.346676447524533\n",
            "Score:  129.91845458642948\n",
            "Score:  21.27904066871617\n",
            "Score:  296.9965024116363\n",
            "Score:  29.836228739839672\n",
            "Score:  46.34682493814572\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 98.6      |\n",
            "|    ep_rew_mean          | 147       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 7         |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 257       |\n",
            "|    total_timesteps      | 2048      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0502394 |\n",
            "|    clip_fraction        | 0.272     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.348    |\n",
            "|    explained_variance   | 0.85      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 18.6      |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | 0.00932   |\n",
            "|    value_loss           | 178       |\n",
            "---------------------------------------\n",
            "Score:  5187.492883417774\n",
            "Score:  15.567550249073072\n",
            "Score:  207.9144996764614\n",
            "Score:  3002.396622766859\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 138        |\n",
            "|    ep_rew_mean          | 503        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 6          |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 454        |\n",
            "|    total_timesteps      | 3072       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08893357 |\n",
            "|    clip_fraction        | 0.314      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.434     |\n",
            "|    explained_variance   | 0.895      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 31.7       |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | 0.025      |\n",
            "|    value_loss           | 250        |\n",
            "----------------------------------------\n",
            "Score:  19.768377769906056\n",
            "Score:  25.604556835233623\n",
            "Score:  107.84115420774725\n",
            "Score:  112.27604413879118\n",
            "Score:  17.652236801182344\n",
            "Score:  1375.286927166491\n",
            "Score:  26.952336805386924\n",
            "Score:  73.04499190950654\n",
            "Score:  23.03297609200035\n",
            "Score:  205.1854913696155\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 127        |\n",
            "|    ep_rew_mean          | 408        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 6          |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 650        |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05033447 |\n",
            "|    clip_fraction        | 0.255      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.229     |\n",
            "|    explained_variance   | 0.811      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 151        |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | 0.113      |\n",
            "|    value_loss           | 725        |\n",
            "----------------------------------------\n",
            "Score:  101.73208335079293\n",
            "Score:  18.272201077597575\n",
            "Score:  80.17575069106849\n",
            "Score:  1059.3697527341976\n",
            "Score:  913.3408172772017\n",
            "Score:  333.29879391194913\n",
            "Score:  59.31649116733623\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 131        |\n",
            "|    ep_rew_mean          | 400        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 6          |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 845        |\n",
            "|    total_timesteps      | 5120       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13391583 |\n",
            "|    clip_fraction        | 0.219      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.336     |\n",
            "|    explained_variance   | 0.925      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 93.5       |\n",
            "|    n_updates            | 230        |\n",
            "|    policy_gradient_loss | 0.00894    |\n",
            "|    value_loss           | 386        |\n",
            "----------------------------------------\n",
            "Score:  24.573233206189613\n",
            "Score:  15.818962322916322\n",
            "Score:  25.509842014403528\n",
            "Score:  35.106189803916195\n",
            "Score:  36.62125067318241\n",
            "Score:  35.17988128151313\n",
            "Score:  204.5672418121772\n",
            "Score:  136.56736458216432\n",
            "Score:  22.43416968219352\n",
            "Score:  39.90152071284682\n",
            "Score:  53.04544615879344\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 117        |\n",
            "|    ep_rew_mean          | 325        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 6          |\n",
            "|    time_elapsed         | 1040       |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.18326929 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.262     |\n",
            "|    explained_variance   | 0.85       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 66.3       |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | 0.0217     |\n",
            "|    value_loss           | 396        |\n",
            "----------------------------------------\n",
            "Score:  1712.1735387928597\n",
            "Score:  485.6921428338252\n",
            "Score:  66.567616721585\n",
            "Score:  15.280257984178776\n",
            "Score:  591.4306781540247\n",
            "Score:  301.55360181083773\n",
            "Score:  283.5260460065342\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 124        |\n",
            "|    ep_rew_mean          | 346        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 1238       |\n",
            "|    total_timesteps      | 7168       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09476256 |\n",
            "|    clip_fraction        | 0.25       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.286     |\n",
            "|    explained_variance   | 0.938      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 32.6       |\n",
            "|    n_updates            | 250        |\n",
            "|    policy_gradient_loss | 0.0227     |\n",
            "|    value_loss           | 372        |\n",
            "----------------------------------------\n",
            "Score:  205.08084375388896\n",
            "Score:  409.039280060354\n",
            "Score:  52.898668747469664\n",
            "Score:  84.18714169438806\n",
            "Score:  665.3742047444313\n",
            "Score:  871.2470944767022\n",
            "Score:  131.2504242372598\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 128        |\n",
            "|    ep_rew_mean          | 346        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 1435       |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11284447 |\n",
            "|    clip_fraction        | 0.23       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.252     |\n",
            "|    explained_variance   | 0.796      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 35.1       |\n",
            "|    n_updates            | 260        |\n",
            "|    policy_gradient_loss | 0.0231     |\n",
            "|    value_loss           | 457        |\n",
            "----------------------------------------\n",
            "Score:  5759.878552120881\n",
            "Score:  17.90220273178688\n",
            "Score:  241.73577526409724\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 134         |\n",
            "|    ep_rew_mean          | 420         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 5           |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 1632        |\n",
            "|    total_timesteps      | 9216        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060807414 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.218      |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 57          |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.0132      |\n",
            "|    value_loss           | 479         |\n",
            "-----------------------------------------\n",
            "Score:  1160.1709640682282\n",
            "Score:  134.1602344946973\n",
            "Score:  324.80284241592\n",
            "Score:  199.57808257428675\n",
            "Score:  279.732699517471\n",
            "Score:  383.77328093755904\n",
            "Eval num_timesteps=10000, episode_reward=331.75 +/- 52.02\n",
            "Episode length: 166.00 +/- 12.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 166         |\n",
            "|    mean_reward          | 332         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 10000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.050934225 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.208      |\n",
            "|    explained_variance   | 0.781       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 35.2        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | 0.0184      |\n",
            "|    value_loss           | 652         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Score:  1106.0819887817752\n",
            "Score:  384.78679520421014\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 140      |\n",
            "|    ep_rew_mean     | 431      |\n",
            "| time/              |          |\n",
            "|    fps             | 5        |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 1867     |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "Score:  48.86364423148409\n",
            "Score:  361.25752459416196\n",
            "Score:  18.59743218160311\n",
            "Score:  613.9261582170066\n",
            "Score:  168.99731323015635\n",
            "Score:  20.99771980049093\n",
            "Score:  43.50245160264061\n",
            "Score:  366.967291631131\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 137         |\n",
            "|    ep_rew_mean          | 408         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 5           |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 2066        |\n",
            "|    total_timesteps      | 11264       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041758608 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.193      |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 34.6        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | 0.00929     |\n",
            "|    value_loss           | 305         |\n",
            "-----------------------------------------\n",
            "Score:  328.8626291429199\n",
            "Score:  48.96703451738693\n",
            "Score:  17.14582690174659\n",
            "Score:  11.501787818663422\n",
            "Score:  11.501787818663422\n",
            "Score:  17.32819177981356\n",
            "Score:  56.13215905856038\n",
            "Score:  17.523208889825128\n",
            "Score:  10.682676311743139\n",
            "Score:  12.196261236039563\n",
            "Score:  32.74601831450361\n",
            "Score:  85.54069395268552\n",
            "Score:  11.463742553080824\n",
            "Score:  10.914722194739738\n",
            "Score:  20.789200779435\n",
            "Score:  22.230382547302863\n",
            "Score:  21.21725067370307\n",
            "Score:  18.446409567182762\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  23.899298706131084\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 121       |\n",
            "|    ep_rew_mean          | 337       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 5         |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 2266      |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.3954761 |\n",
            "|    clip_fraction        | 0.295     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.227    |\n",
            "|    explained_variance   | 0.767     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 50.6      |\n",
            "|    n_updates            | 300       |\n",
            "|    policy_gradient_loss | 0.0416    |\n",
            "|    value_loss           | 372       |\n",
            "---------------------------------------\n",
            "Score:  10.914722194739738\n",
            "Score:  16.328306678990945\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  16.328306678990945\n",
            "Score:  16.328306678990945\n",
            "Score:  16.328306678990945\n",
            "Score:  16.328306678990945\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  17.32819177981356\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  16.328306678990945\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  10.682676311743139\n",
            "Score:  16.328306678990945\n",
            "Score:  10.682676311743139\n",
            "Score:  16.328306678990945\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 95.7      |\n",
            "|    ep_rew_mean          | 215       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 5         |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 2464      |\n",
            "|    total_timesteps      | 13312     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.5692679 |\n",
            "|    clip_fraction        | 0.183     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.0494   |\n",
            "|    explained_variance   | 0.12      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.37      |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | 0.0786    |\n",
            "|    value_loss           | 104       |\n",
            "---------------------------------------\n",
            "Score:  12.34802524274933\n",
            "Score:  13.93019338752097\n",
            "Score:  37.838834961191466\n",
            "Score:  19.5115685619802\n",
            "Score:  15.823828015564317\n",
            "Score:  19.205372276959277\n",
            "Score:  36.74638646771326\n",
            "Score:  16.267588621972205\n",
            "Score:  13.086260392091143\n",
            "Score:  16.28362450413368\n",
            "Score:  29.677485935741363\n",
            "Score:  13.08789590942726\n",
            "Score:  42.69322945081683\n",
            "Score:  21.351028667227897\n",
            "Score:  158.75050975376104\n",
            "Score:  77.9301006516727\n",
            "Score:  13.933814026975561\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 88.2       |\n",
            "|    ep_rew_mean          | 190        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 2663       |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.34098768 |\n",
            "|    clip_fraction        | 0.128      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.0917    |\n",
            "|    explained_variance   | -0.856     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.135      |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -0.0249    |\n",
            "|    value_loss           | 10.1       |\n",
            "----------------------------------------\n",
            "Score:  176.4168274849942\n",
            "Score:  58.83123214012288\n",
            "Score:  21.218922314926647\n",
            "Score:  40.252264016652255\n",
            "Score:  16.596403258676876\n",
            "Score:  12.31902982039496\n",
            "Score:  13.935676064061013\n",
            "Score:  107.94369736868322\n",
            "Score:  41.29242058655858\n",
            "Score:  26.429285969336743\n",
            "Score:  21.07379516631962\n",
            "Score:  17.46095896203171\n",
            "Score:  26.671135219445947\n",
            "Score:  68.08401936984028\n",
            "Score:  18.6213803890126\n",
            "Score:  16.14440175155004\n",
            "Score:  13.795211832161478\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 75         |\n",
            "|    ep_rew_mean          | 144        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 2862       |\n",
            "|    total_timesteps      | 15360      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08648324 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.208     |\n",
            "|    explained_variance   | 0.739      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.707      |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | 0.0441     |\n",
            "|    value_loss           | 15.6       |\n",
            "----------------------------------------\n",
            "Score:  171.7220305792336\n",
            "Score:  32.84834052941687\n",
            "Score:  27.238872661103095\n",
            "Score:  17.12883363347265\n",
            "Score:  22.742285659940876\n",
            "Score:  31.59160051806091\n",
            "Score:  25.966457506002687\n",
            "Score:  47.084559502894976\n",
            "Score:  55.18966503949166\n",
            "Score:  62.99973821344865\n",
            "Score:  17.495272793186782\n",
            "Score:  18.265286636348552\n",
            "Score:  29.016082399965082\n",
            "Score:  163.4628585202556\n",
            "Score:  16.331558107487062\n",
            "Score:  19.34190150097132\n",
            "Score:  16.281021071584014\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 55        |\n",
            "|    ep_rew_mean          | 35.7      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 5         |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 3061      |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0632431 |\n",
            "|    clip_fraction        | 0.176     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.246    |\n",
            "|    explained_variance   | 0.788     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.18      |\n",
            "|    n_updates            | 340       |\n",
            "|    policy_gradient_loss | 0.0125    |\n",
            "|    value_loss           | 11.2      |\n",
            "---------------------------------------\n",
            "Score:  20.177080933553142\n",
            "Score:  16.297044877381932\n",
            "Score:  20.396462118691627\n",
            "Score:  22.69756860844946\n",
            "Score:  20.046708303388897\n",
            "Score:  17.399893997403492\n",
            "Score:  29.480977076391994\n",
            "Score:  16.410941070451493\n",
            "Score:  81.9216395893251\n",
            "Score:  17.35515329293918\n",
            "Score:  40.0614554014139\n",
            "Score:  30.045527148823705\n",
            "Score:  19.055815222804693\n",
            "Score:  38.328173990727116\n",
            "Score:  38.09559888300595\n",
            "Score:  16.328306678990945\n",
            "Score:  16.640376824511893\n",
            "Score:  13.094207296268456\n",
            "Score:  16.299717274776032\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 52.8       |\n",
            "|    ep_rew_mean          | 29.1       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 3259       |\n",
            "|    total_timesteps      | 17408      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09498923 |\n",
            "|    clip_fraction        | 0.224      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.257     |\n",
            "|    explained_variance   | 0.749      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.03       |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | 0.0202     |\n",
            "|    value_loss           | 13.6       |\n",
            "----------------------------------------\n",
            "Score:  17.474848453216293\n",
            "Score:  49.51926473235979\n",
            "Score:  33.351957842510714\n",
            "Score:  211.88430155073033\n",
            "Score:  43.87628708528307\n",
            "Score:  16.17482244841782\n",
            "Score:  50.53544070872587\n",
            "Score:  15.673588687140748\n",
            "Score:  148.17252515299955\n",
            "Score:  22.54789347222076\n",
            "Score:  31.12491903006608\n",
            "Score:  22.171880559146818\n",
            "Score:  61.71465527263433\n",
            "Score:  104.74600454792359\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 56.9       |\n",
            "|    ep_rew_mean          | 35.4       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 5          |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 3458       |\n",
            "|    total_timesteps      | 18432      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08880015 |\n",
            "|    clip_fraction        | 0.212      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.223     |\n",
            "|    explained_variance   | 0.749      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.13       |\n",
            "|    n_updates            | 360        |\n",
            "|    policy_gradient_loss | 0.00354    |\n",
            "|    value_loss           | 4.59       |\n",
            "----------------------------------------\n",
            "Score:  480.1306767616632\n",
            "Score:  34.079671437614586\n",
            "Score:  26.646310891924298\n",
            "Score:  183.6965606280409\n",
            "Score:  87.85910058871916\n",
            "Score:  319.3405742594306\n",
            "Score:  388.13190439872284\n",
            "Score:  88.07353123737158\n",
            "Score:  30.611546274768685\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 63.8        |\n",
            "|    ep_rew_mean          | 50.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 5           |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 3657        |\n",
            "|    total_timesteps      | 19456       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.114312425 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.292      |\n",
            "|    explained_variance   | 0.813       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.63        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | 0.0105      |\n",
            "|    value_loss           | 14.1        |\n",
            "-----------------------------------------\n",
            "Score:  280.4165508559635\n",
            "Score:  28.007807027190193\n",
            "Score:  362.7183053281794\n",
            "Score:  933.1027498997325\n",
            "Score:  1993.4919114173515\n",
            "Eval num_timesteps=20000, episode_reward=1463.30 +/- 530.19\n",
            "Episode length: 326.50 +/- 58.50\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 326        |\n",
            "|    mean_reward          | 1.46e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 20000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09428221 |\n",
            "|    clip_fraction        | 0.285      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.259     |\n",
            "|    explained_variance   | 0.699      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.28       |\n",
            "|    n_updates            | 380        |\n",
            "|    policy_gradient_loss | 0.0252     |\n",
            "|    value_loss           | 37.8       |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Score:  720.4334134382651\n",
            "Score:  57.101864368426234\n",
            "Score:  64.86540627815684\n",
            "Score:  47.18580967412756\n",
            "Score:  69.2360213722164\n",
            "Score:  178.5823330519272\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 70.7     |\n",
            "|    ep_rew_mean     | 67.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 5        |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 3907     |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "Training completed!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7a0fb4d2e710>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}